{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"14pxj3QfwegBblh0PDyDjM83r-g6bBDH5","authorship_tag":"ABX9TyOCfKWIK5PBCOhji9B+nWc6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F1JFmYz9VJGK"},"outputs":[],"source":["#===============================================================\n","# DEBUG TOGGLE SWITCH\n","def d(*args, **kwargs):\n","    if d.ON:\n","        print(*args, **kwargs)\n","d.ON = True\n","#===============================================================\n","\n","#================================================\n","if __name__ == \"__main__\":\n","  try:\n","    import segmentation_models_pytorch as smp\n","  except ImportError:\n","    !pip install -q segmentation_models_pytorch timm\n","    import segmentation_models_pytorch as smp\n","#============================================================0\n","\n","#=============================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torchvision import transforms as T\n","from PIL import Image\n","from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score, roc_auc_score, roc_curve\n","import pandas as pd\n","from tqdm import tqdm\n","#===================================================================\n","\n","\n","#=====================================================================00\n","# PATHS SETTING\n","root_dir = \"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection\"\n","frames_dir = os.path.join(root_dir, \"Datasets/RoadAnomaly_jpg\")\n","split_file = os.path.join(root_dir, \"ProjectWorkspace/splits/roadAnomaly_valid_pairs.txt\")\n","ckpt_path = os.path.join(root_dir, \"ProjectWorkspace/ckpts/updated_loss_model.pth\")\n","lambda_path = os.path.join(root_dir, \"ProjectWorkspace/src/eval/lambda_hat_3rdattempt.txt\")\n","output_csv = os.path.join(root_dir, \"ProjectWorkspace/eval/roadAnomaly_metrics.csv\")\n","#================================================================================0\n","\n","#==================================================================================\n","# NAVIGATION TO GET TO THE FOLDER WITH STUFF INSIDE\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/src\")\n","\n","#==============================================================================\n","# MODEL LOADING\n","from network.deeplab_dualhead import get_model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = get_model().to(device)\n","model.load_state_dict(torch.load(ckpt_path, map_location = device))\n","model.eval()\n","#=================================================================================\n","\n","#================================================================================\n","# CALIB LOADING\n","with open(lambda_path, \"r\") as f:\n","    lambda_hat = float(f.read().strip())\n","d(f\"Loaded lambda_hat = {lambda_hat}\")\n","#===================================================================================\n","\n","\n","#===================================================================================\n","# PREPROCESSING\n","transform = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225])\n","])\n","#==================================================================================0\n","\n","\n","#======================================================================\n","# INFERENCE\n","threshold_objectness = 0.51\n","results = []\n","all_gt_pixels = []\n","all_scores = []\n","\n","with open(split_file, \"r\") as f:\n","  rel_paths = f.read().splitlines()\n","\n","for rel_path in tqdm(rel_paths):\n","  rgb_path = os.path.join(frames_dir, rel_path)\n","  label_path = rgb_path.replace(\".jpg\", \".labels/labels_semantic.png\")\n","\n","  rgb_img = Image.open(rgb_path).convert(\"RGB\")\n","  input_tensor = transform(rgb_img).unsqueeze(0).to(device)\n","\n","  with torch.no_grad():\n","    seg_logits, obj_logits = model(input_tensor)\n","    seg_logits = F.interpolate(seg_logits, size=rgb_img.size[::-1], mode=\"bilinear\", align_corners=False)\n","    obj_logits = F.interpolate(obj_logits, size=rgb_img.size[::-1], mode=\"bilinear\", align_corners=False)\n","\n","    softmax = torch.softmax(seg_logits, dim=1)\n","    objectness = torch.sigmoid(obj_logits)\n","\n","    conf_score, _ = torch.max(softmax, dim=1)\n","    nonconformity = 1.0 - conf_score\n","    unknown_mask = (nonconformity > lambda_hat).squeeze(0).cpu().numpy()\n","    object_mask = (objectness > threshold_objectness).squeeze().cpu().numpy()\n","    obstacle_mask = (unknown_mask & object_mask).astype(np.uint8)\n","    obstacle_score = objectness.squeeze().cpu().numpy() * nonconformity.squeeze().cpu().numpy()\n","\n","  gt_mask = np.array(Image.open(label_path))\n","  gt_anomaly = (gt_mask == 2).astype(np.uint8)\n","\n","  pred_flat = obstacle_mask.flatten()\n","  gt_flat = gt_anomaly.flatten()\n","  score_flat = obstacle_score.flatten()\n","\n","  precision = precision_score(gt_flat, pred_flat, zero_division=0)\n","  recall = recall_score(gt_flat, pred_flat, zero_division=0)\n","  f1 = f1_score(gt_flat, pred_flat, zero_division=0)\n","  iou = jaccard_score(gt_flat, pred_flat, zero_division=0)\n","\n","  results.append({\n","      \"image\": rel_path,\n","      \"precision\": precision,\n","      \"recall\": recall,\n","      \"f1\": f1,\n","      \"iou\": iou\n","  })\n","\n","  all_gt_pixels.append(gt_flat)\n","  all_scores.append(score_flat)\n","\n","df = pd.DataFrame(results)\n","df.to_csv(output_csv, index=False)\n","d(f\"Saved metrics to {output_csv}\")\n","#=============================================================================\n","\n","\n","#===========================================================================0\n","# AGGREGATED METRICS â€” FULL vs. TRIMMED\n","metrics = [\"precision\", \"recall\", \"f1\", \"iou\"]\n","\n","# GLOBAL\n","global_stats = df[metrics].agg([\"mean\", \"std\", \"min\", \"max\", \"median\"])\n","d(\"\\n== GLOBAL METRICS (all images) ==\")\n","d(global_stats)\n","\n","# GLOBAL SCORE-BASED\n","all_gt_pixels = np.concatenate(all_gt_pixels)\n","all_scores = np.concatenate(all_scores)\n","\n","# AUROC\n","auroc = roc_auc_score(all_gt_pixels, all_scores)\n","# FPR@95TPR\n","fpr, tpr, thresholds = roc_curve(all_gt_pixels, all_scores)\n","fpr95 = fpr[np.argmax(tpr >= 0.95)]\n","\n","d(f\"\\n== GLOBAL SCORE-BASED METRICS ==\")\n","d(f\"AUROC = {auroc:.6f}\")\n","d(f\"FPR@95TPR = {fpr95:.6f}\")\n","#==============================================================================\n","\n","\n","# ONE SIDED LOWER ALPHA-TRIMMED EVALUATION  (about IoU):\n","#we noticed a lot of images are not reliable (RoadAnomaly is highly extreme in terms of OOD data wrt to training data we had to use) to perform inference, based on Cityscapes training, for example rural areas looking very different from cityscapes\n","alpha = 0.30\n","q_alpha = df[\"iou\"].quantile(alpha)\n","df_trimmed = df[df[\"iou\"] > q_alpha]\n","d(f\"\\nExcluded {len(df) - len(df_trimmed)} samples (bottom {int(alpha*100)}% IoU)\")\n","trimmed_stats = df_trimmed[metrics].agg([\"mean\", \"std\", \"min\", \"max\", \"median\"])\n","d(f\"== TRIMMED METRICS (top {100 - int(alpha*100)}% samples) ==\")\n","d(trimmed_stats)\n","#=====================================================================\n","\n"]},{"cell_type":"code","source":["#============================================================\n","# DISPLAY PREDICTIONS FOR PRESENTATION\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","from matplotlib.colors import LinearSegmentedColormap\n","\n","save_outputs = True # toggling until they're good to be saved\n","\n","metrics_df = pd.read_csv(output_csv)\n","\n","q_alpha = metrics_df[\"iou\"].quantile(alpha) #visibility as variable, high cohesion low coupling\n","q_top10 = metrics_df[\"iou\"].quantile(0.9)\n","\n","top_samples = set(metrics_df[metrics_df[\"iou\"] > q_top10][\"image\"])\n","good_samples = set(metrics_df[metrics_df[\"iou\"] > q_alpha][\"image\"])\n","\n","save_dir_root = os.path.join(root_dir, \"ProjectWorkspace/eval/RoadAnomalyInferenceImages\")\n","save_top_root = os.path.join(save_dir_root, \"TopSamples\")\n","save_trimmed_root = os.path.join(save_dir_root, \"TrimmedSamples\")\n","os.makedirs(save_dir_root, exist_ok=True)\n","os.makedirs(save_top_root, exist_ok=True)\n","os.makedirs(save_trimmed_root, exist_ok=True)\n","\n","for sample in results:\n","  rel_path = sample[\"image\"]\n","  rgb_path = os.path.join(frames_dir, rel_path)\n","  label_path = rgb_path.replace(\".jpg\", \".labels/labels_semantic.png\")\n","  name = os.path.splitext(os.path.basename(rgb_path))[0]\n","\n","  is_top = rel_path in top_samples\n","  is_good = rel_path in good_samples\n","  should_save = save_outputs\n","  if should_save:\n","    if is_top:\n","      save_dir = os.path.join(save_top_root, name) #save_dir assumes this value for top samples (need em to show in which conditions the model performs well and the uncertainty is low)\n","    elif is_good:\n","      save_dir = os.path.join(save_dir_root, name) #save_dir assumes this value for good samples\n","    else:\n","      save_dir = os.path.join(save_trimmed_root, name) #save_dir assumes this value for trimmed samples (need em as well to be shown in the presentation and justify the choice)\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","  rgb_img = Image.open(rgb_path).convert(\"RGB\")\n","\n","  gt_mask = np.array(Image.open(label_path))\n","  gt_anomaly = (gt_mask == 2).astype(np.uint8)\n","\n","  input_tensor = transform(rgb_img).unsqueeze(0).to(device)\n","  with torch.no_grad():\n","      seg_logits, obj_logits = model(input_tensor)\n","      seg_logits = F.interpolate(seg_logits, size=rgb_img.size[::-1], mode=\"bilinear\", align_corners=False)\n","      obj_logits = F.interpolate(obj_logits, size=rgb_img.size[::-1], mode=\"bilinear\", align_corners=False)\n","      softmax = torch.softmax(seg_logits, dim=1)\n","      objectness = torch.sigmoid(obj_logits)\n","      conf_score, _ = torch.max(softmax, dim=1)\n","      nonconformity = 1.0 - conf_score\n","      unknown_mask = (nonconformity > lambda_hat).squeeze(0).cpu().numpy()\n","      object_mask = (objectness > threshold_objectness).squeeze().cpu().numpy()\n","      obstacle_mask = (unknown_mask & object_mask).astype(np.uint8)\n","      conf_map = conf_score.squeeze().cpu().numpy()\n","      obstacle_score = (objectness.squeeze().cpu().numpy()) * (nonconformity.squeeze().cpu().numpy())\n","\n","\n","  # VISUAL DISPLAY (always)\n","  fig, axs = plt.subplots(1, 7, figsize=(42, 6))\n","  axs[0].imshow(rgb_img)\n","  axs[0].set_title(\"RGB Image\")\n","  axs[1].imshow(gt_anomaly, cmap=\"gray\")\n","  axs[1].set_title(\"GT Anomaly\")\n","  axs[2].imshow(unknown_mask, cmap=\"gray\")\n","  axs[2].set_title(\"Unknown Mask\")\n","  axs[3].imshow(object_mask, cmap=\"gray\")\n","  axs[3].set_title(\"Objectness Mask\")\n","  axs[4].imshow(obstacle_mask, cmap=\"gray\")\n","  axs[4].set_title(\"Obstacle = Unknown AND Object\")\n","\n","  # CONFIDENCE MAP USING ONLY SOFTMAX\n","  im1 = axs[5].imshow(conf_map, cmap=\"viridis\", vmin=0.0, vmax=1.0)\n","  axs[5].set_title(\"Confidence Map (Max Softmax)\")\n","  cbar1 = fig.colorbar(im1, ax=axs[5], fraction=0.046, pad=0.04)\n","  cbar1.set_label(\"Confidence scale\")\n","  # CONFIDENCE MAP USING SOFTMAX AND SIGMOID HEAD COMBINED\n","  im2 = axs[6].imshow(obstacle_score, cmap=\"viridis_r\", vmin=0.0, vmax=1.0)  #doing viridis\"_r\" in order to revert the behaviour of the viridis heatmap and return an image prone to be compared to the other confidence map\n","  axs[6].set_title(\"Obstacle Score = Objectness combined with Nonconformity\")\n","  cbar2 = fig.colorbar(im2, ax=axs[6], fraction=0.046, pad=0.04)\n","  cbar2.set_label(\"Obstacle Score\")\n","\n","  for ax in axs:\n","      ax.axis(\"off\")\n","  plt.suptitle(name, fontsize=18)\n","  plt.tight_layout()\n","  plt.show()\n","\n","  # PER-IMAGE SAVING (only if save_outputs and in top samples)\n","  if should_save:\n","      rgb_img.save(os.path.join(save_dir, \"rgb.png\"))\n","\n","      #Softmax confidence map saving\n","      fig, ax = plt.subplots(figsize=(6, 5))\n","      im = ax.imshow(conf_map, cmap=\"viridis\", vmin=0.0, vmax=1.0)\n","      ax.axis(\"off\")\n","      cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","      cbar.set_label(\"Confidence scale\")\n","      plt.savefig(os.path.join(save_dir, \"confidence_map.png\"), dpi=200)\n","      plt.close()\n","\n","      #Softmax and sigmoid confidence map saving\n","      fig, ax = plt.subplots(figsize=(6, 5))\n","      im = ax.imshow(obstacle_score, cmap=\"viridis_r\", vmin=0.0, vmax=1.0)\n","      ax.axis(\"off\")\n","      cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","      cbar.set_label(\"Obstacle Score\")\n","      plt.savefig(os.path.join(save_dir, \"obstacle_score.png\"), dpi=200)\n","      plt.close()\n","\n","      if is_good: #additional masks only for good samples\n","        Image.fromarray((gt_anomaly * 255).astype(np.uint8)).save(os.path.join(save_dir, \"gt_anomaly.png\"))\n","        Image.fromarray((unknown_mask * 255).astype(np.uint8)).save(os.path.join(save_dir, \"unknown_mask.png\"))\n","        Image.fromarray((object_mask * 255).astype(np.uint8)).save(os.path.join(save_dir, \"object_mask.png\"))\n","        Image.fromarray((obstacle_mask * 255).astype(np.uint8)).save(os.path.join(save_dir, \"obstacle_mask.png\"))\n","#===============================================================\n"],"metadata":{"id":"cO6wb1V8dusd"},"execution_count":null,"outputs":[]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UDYwFn2poMcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/SAPIENZA/CV/Project/RoadObstaceDetection"
      ],
      "metadata": {
        "id": "Z1Dn9t5-p2x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs the segmentation-models-pytorch library used for DeepLabV3+\n",
        "!pip install -U segmentation-models-pytorch --quiet\n"
      ],
      "metadata": {
        "id": "nUGVtPC6I9FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import random\n",
        "from scipy.ndimage import binary_opening, binary_closing, binary_dilation\n"
      ],
      "metadata": {
        "id": "4avVG5nEIb4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Maps fine-grained Cityscapes/LostAndFound labels to 7 macro-classes\n",
        "CITYSCAPES_LABELIDS_TO_GROUP = {\n",
        "\n",
        "    0: 0,      # road\n",
        "    1: 1, 2: 1,                   # sidewalk, parking -> flat\n",
        "    3: 4, 4: 4, 5: 4, 6: 4, 7: 4, # building, wall, fence, guard rail, bridge -> construction\n",
        "    8: 5, 9: 5, 10: 5,            # pole, traffic light, traffic sign -> object\n",
        "    11: 2, 12: 2,                 # person, rider -> human\n",
        "    13: 3, 14: 3, 15: 3, 16: 3, 17: 3, 18: 3, # car, truck, bus, caravan, trailer, train -> vehicle\n",
        "    19: 6, 20: 6, 21: 6, 22: 6, 23: 6, 24: 6, 255: 6, # sky, ground, static, dynamic, water, terrain, unlabeled -> background\n",
        "    40:5, 41:5, 42:5\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "_2HmIAT1Hf0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MappedObstacleDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, img_transform=None, mask_transform=None):\n",
        "        self.img_paths = []\n",
        "        self.mask_paths = []\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        for city in os.listdir(img_dir):\n",
        "            city_img_dir = os.path.join(img_dir, city)\n",
        "            city_mask_dir = os.path.join(mask_dir, city)\n",
        "            if not os.path.isdir(city_img_dir): continue\n",
        "            for fname in os.listdir(city_img_dir):\n",
        "                if fname.endswith(\"_leftImg8bit.png\"):\n",
        "                    img_path = os.path.join(city_img_dir, fname)\n",
        "                    mask_name = fname.replace(\"_leftImg8bit.png\", \"_gtCoarse_labelIds.png\")\n",
        "                    mask_path = os.path.join(city_mask_dir, mask_name)\n",
        "                    if os.path.exists(mask_path):\n",
        "                        self.img_paths.append(img_path)\n",
        "                        self.mask_paths.append(mask_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        mask = np.array(Image.open(self.mask_paths[idx]))  # Cityscapes labelIds\n",
        "\n",
        "        # Mapping\n",
        "        macro_mask = np.vectorize(lambda x: CITYSCAPES_LABELIDS_TO_GROUP.get(x, 255))(mask).astype(np.int64)\n",
        "\n",
        "\n",
        "        macro_mask = torch.from_numpy(macro_mask).long()\n",
        "\n",
        "        # OBJECTNESS: 1 where macroclass == 2,3,4,5 (\"object\"), 0 elsewhere\n",
        "        object_mask = ((macro_mask == 2)| (macro_mask == 3) | (macro_mask == 4) | (macro_mask == 5)).long()\n",
        "\n",
        "        # Transforms\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        if self.mask_transform:\n",
        "           print(\"📏 object_mask shape:\", object_mask.shape, \"dtype:\", object_mask.dtype)\n",
        "\n",
        "           # Convert to numpy uint8 before PIL conversion\n",
        "           object_mask_np = object_mask.numpy().astype(np.uint8) * 255\n",
        "           object_mask_pil = Image.fromarray(object_mask_np)\n",
        "           object_mask = self.mask_transform(object_mask_pil).squeeze(0)\n",
        "\n",
        "           macro_mask = self.mask_transform(Image.fromarray(macro_mask.numpy().astype(np.uint8) * 32)).squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"class_mask\": macro_mask,\n",
        "            \"object_mask\": object_mask\n",
        "        }\n",
        "\n",
        "resize_dims = (512, 1024)\n",
        "img_transform = T.Compose([\n",
        "    T.Resize(resize_dims),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ===  Transforms  ===\n",
        "mask_transform = T.Compose([\n",
        "    T.Resize((512, 1024), interpolation=T.InterpolationMode.NEAREST),\n",
        "    T.ToTensor(),  # Convert to tensor [1, H, W] float32 ∈ [0, 1]\n",
        "    T.Lambda(lambda x: (x > 0.5).long())  # Binarization→ 0 o 1, type long\n",
        "])\n",
        "# == DATASET/LOADER == #\n",
        "img_dir = \"/content/drive/MyDrive/SAPIENZA/CV/Project/RoadObstacleDetection/Datasets/LostAndFound/leftImg8bit/test\"\n",
        "mask_dir = \"/content/drive/MyDrive/SAPIENZA/CV/Project/RoadObstacleDetection/Datasets/LostAndFound/gtCoarse/test\"\n",
        "\n",
        "test_dataset = MappedObstacleDataset(\n",
        "    img_dir=img_dir,\n",
        "    mask_dir=mask_dir,\n",
        "    img_transform=img_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "Wets1vGnHvKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepLabDualHead(nn.Module):\n",
        "    def __init__(self, n_classes=7):\n",
        "        super().__init__()\n",
        "        self.base = smp.DeepLabV3Plus(\n",
        "            encoder_name=\"resnet50\",\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=3,\n",
        "            classes=n_classes\n",
        "        )\n",
        "        self.base.classifier = nn.Identity()\n",
        "\n",
        "        self.head_softmax = nn.Conv2d(256, n_classes, kernel_size=1)\n",
        "        self.head_sigmoid = nn.Sequential(\n",
        "            nn.Conv2d(256, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base.encoder(x)\n",
        "        decoder_output = self.base.decoder(features)\n",
        "        softmax_output = self.head_softmax(decoder_output)\n",
        "        sigmoid_output = self.head_sigmoid(decoder_output)\n",
        "        return softmax_output, sigmoid_output\n",
        "\n"
      ],
      "metadata": {
        "id": "G9zZBiGssWh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepLabDualHead(n_classes=7)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/SAPIENZA/CV/Project/RoadObstacleDetection/ProjectWorkspace/ckpts/updated_loss_model.pth\", map_location=device))\n",
        "model = model.eval().to(device)"
      ],
      "metadata": {
        "id": "aMVdA9NKXlnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Applies thresholding and morphological filtering to a UOS map.\n",
        "# Filters small blobs, removes overly large components, and optionally dilates valid predictions to cover full obstacles.\n",
        "\n",
        "# Applies adaptive or fixed thresholding to the UOS map,\n",
        "# followed by morphological operations to clean small noisy regions\n",
        "# and remove overly large connected components (non-obstacles).\n",
        "# Optionally, it performs morphological dilation to extend the predicted blobs,\n",
        "# making the obstacle masks more complete (especially useful for small or fragmented detections).\n",
        "\n",
        "def filter_and_threshold(uos_map, adaptive=True, threshold=0.1, top_percent=0.02,\n",
        "                         open_size=1, close_size=2, max_area_ratio=0.03,\n",
        "                         dilate=True, dilate_size=5):\n",
        "    if adaptive:\n",
        "        thresh = np.quantile(uos_map, 1 - top_percent)\n",
        "    else:\n",
        "        thresh = threshold\n",
        "\n",
        "    bin_mask = uos_map > thresh\n",
        "\n",
        "    # Morphology\n",
        "    bin_mask = binary_opening(bin_mask, structure=np.ones((open_size, open_size)))\n",
        "    bin_mask = binary_closing(bin_mask, structure=np.ones((close_size, close_size)))\n",
        "\n",
        "    # Remove too big components\n",
        "    bin_mask_uint8 = (bin_mask * 255).astype(np.uint8)\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(bin_mask_uint8, connectivity=8)\n",
        "\n",
        "    H, W = uos_map.shape\n",
        "    max_area = max_area_ratio * H * W\n",
        "    clean_mask = np.zeros_like(bin_mask_uint8)\n",
        "\n",
        "    for i in range(1, num_labels):  # 0 = background\n",
        "        area = stats[i, cv2.CC_STAT_AREA]\n",
        "        if area <= max_area:\n",
        "            clean_mask[labels == i] = 1\n",
        "\n",
        "    # Dilation to extend detected blobs\n",
        "    if dilate:\n",
        "        clean_mask = binary_dilation(clean_mask, structure=np.ones((dilate_size, dilate_size))).astype(np.uint8)\n",
        "\n",
        "    return clean_mask\n",
        "\n",
        "# Visualizes a few random predictions from the dataset using UOS maps.\n",
        "# Applies uncertainty and objectness thresholds, filtering, and creates overlay visualizations against the ground truth.\n",
        "def visualize_random_predictions2(model, dataset, device,\n",
        "                                 lambda_hat=0.6, obj_thresh=0.1, top_percent=0.02,\n",
        "                                 num_samples=4, adaptive=True):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        rand_indices = random.sample(range(len(dataset)), num_samples)\n",
        "        batch = [dataset[i] for i in rand_indices]\n",
        "\n",
        "        images = torch.stack([item[\"image\"] for item in batch]).to(device)\n",
        "        targets = torch.stack([item[\"object_mask\"] for item in batch])\n",
        "\n",
        "        # Forward step\n",
        "        softmax_output, sigmoid_output = model(images)\n",
        "        class_probs = torch.softmax(softmax_output, dim=1)\n",
        "        objectness = sigmoid_output.squeeze(1)\n",
        "        max_probs = class_probs.max(dim=1)[0]\n",
        "        nonconf = 1.0 - max_probs\n",
        "\n",
        "        uncertainty_mask = (nonconf > lambda_hat).float()\n",
        "        object_mask = (objectness > obj_thresh).float()\n",
        "        uos_map = (uncertainty_mask * object_mask).cpu().numpy()\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if targets[i].sum() == 0:\n",
        "                continue\n",
        "\n",
        "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "            img = np.clip(img, 0, 1)\n",
        "            img_uint8 = (img * 255).astype(np.uint8)\n",
        "\n",
        "            gt_mask = targets[i].cpu().numpy()\n",
        "            uos_pred = uos_map[i]\n",
        "\n",
        "            # UOS filter with dilation\n",
        "            pred_mask = filter_and_threshold(\n",
        "                uos_pred,\n",
        "                adaptive=adaptive,\n",
        "                top_percent=top_percent,\n",
        "                open_size=1,\n",
        "                close_size=2,\n",
        "                max_area_ratio=0.05,\n",
        "                dilate=True,             #  dilation enabling\n",
        "                dilate_size=4           #  kernel 5x5 (can actually change)\n",
        "            )\n",
        "\n",
        "            H, W = img_uint8.shape[:2]\n",
        "            gt_resized = cv2.resize(gt_mask.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "            pred_resized = cv2.resize(pred_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # Overlay\n",
        "            overlay = np.zeros_like(img_uint8)\n",
        "            overlay[gt_resized == 1] = [255, 0, 0]\n",
        "            overlay[pred_resized == 1] = [0, 255, 0]\n",
        "            overlay[(gt_resized == 1) & (pred_resized == 1)] = [255, 255, 0]\n",
        "            blended = cv2.addWeighted(img_uint8, 1.0, overlay, 0.5, 0)\n",
        "\n",
        "            # Plot\n",
        "            plt.figure(figsize=(20, 5))\n",
        "            plt.subplot(1, 5, 1); plt.imshow(img); plt.title(\"Image\"); plt.axis('off')\n",
        "            plt.subplot(1, 5, 2); plt.imshow(gt_resized, cmap='gray'); plt.title(\"GT Obstacle\"); plt.axis('off')\n",
        "            plt.subplot(1, 5, 3); plt.imshow(objectness[i].cpu(), cmap='hot'); plt.title(\"Objectness\"); plt.axis('off')\n",
        "            plt.subplot(1, 5, 4); plt.imshow(uos_pred, cmap='hot'); plt.title(\"UOS map\"); plt.axis('off')\n",
        "            plt.subplot(1, 5, 5); plt.imshow(blended); plt.title(\"Overlay\"); plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "kY8RrDmxseKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_random_predictions2(\n",
        "    model,\n",
        "    dataset=test_loader.dataset,\n",
        "    device=device,\n",
        "    lambda_hat=0.6,\n",
        "    obj_thresh=0.2,\n",
        "    top_percent=0.02,\n",
        "    num_samples=4,\n",
        "    adaptive=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "bw7WZvXMmTNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs a grid search over lambda_hat and objectness threshold values.\n",
        "# For each combination, evaluates prediction performance and saves results in a CSV file for comparison.\n",
        "def analyze_metrics_grid(model,\n",
        "                         dataset,\n",
        "                         device,\n",
        "                         lambda_values=[0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "                         obj_thresh_values=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "                         max_images=5,\n",
        "                         save_dir=\"grid_search_results\"):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    results = []\n",
        "\n",
        "    for lambda_hat in tqdm(lambda_values, desc=\"λ sweep\"):\n",
        "        for obj_thresh in obj_thresh_values:\n",
        "            try:\n",
        "                print(f\"\\n🔍 Testing λ={lambda_hat}, obj_thresh={obj_thresh}\")\n",
        "                result_df = analyze_uos_predictions(\n",
        "                    model=model,\n",
        "                    dataset=dataset,\n",
        "                    device=device,\n",
        "                    lambda_hat=lambda_hat,\n",
        "                    obj_thresh=obj_thresh,\n",
        "                    max_images=max_images,\n",
        "                    save_dir=os.path.join(save_dir, f\"lambda_{lambda_hat}_obj_{obj_thresh}\")\n",
        "                )\n",
        "\n",
        "                precision_mean = result_df[\"Precision\"].mean()\n",
        "                recall_mean = result_df[\"Recall\"].mean()\n",
        "                iou_mean = result_df[\"IoU\"].mean()\n",
        "\n",
        "                results.append({\n",
        "                    \"lambda_hat\": lambda_hat,\n",
        "                    \"obj_thresh\": obj_thresh,\n",
        "                    \"Precision\": precision_mean,\n",
        "                    \"Recall\": recall_mean,\n",
        "                    \"IoU\": iou_mean\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error at λ={lambda_hat}, obj_thresh={obj_thresh}: {e}\")\n",
        "                results.append({\n",
        "                    \"lambda_hat\": lambda_hat,\n",
        "                    \"obj_thresh\": obj_thresh,\n",
        "                    \"Precision\": None,\n",
        "                    \"Recall\": None,\n",
        "                    \"IoU\": None\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(save_dir, \"grid_metrics.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n Grid search results saved to: {csv_path}\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Dn38Y0HOdKrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_results_df, grid_results_csv = analyze_metrics_grid(\n",
        "    model=model,\n",
        "    dataset=test_loader.dataset,\n",
        "    device=device,\n",
        "    lambda_values=[0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    obj_thresh_values=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    max_images=100,\n",
        "    save_dir=\"/mnt/data/grid_search_results\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "BBrY61RZW-CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full evaluation using UOS maps with adaptive thresholding and morphological filtering (including dilation).\n",
        "# Computes pixel-wise metrics, generates visualization maps, and saves per-image results and global summary.\n",
        "def analyze_uos_predictions_filtered(model, dataset, device,\n",
        "                                     lambda_hat=0.6, obj_thresh=0.1, top_percent=0.05,\n",
        "                                     adaptive=True,\n",
        "                                     open_size=1, close_size=2, max_area_ratio=0.05,\n",
        "                                     dilate=True, dilate_size=4,\n",
        "                                     save_dir=\"uos_eval_filtered\"):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"overlays\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"maps\"), exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_preds, all_targets, metrics = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(len(dataset))):\n",
        "            sample = dataset[idx]\n",
        "            image = sample[\"image\"].unsqueeze(0).to(device)\n",
        "            gt_mask = sample[\"object_mask\"].numpy()\n",
        "\n",
        "            softmax_output, sigmoid_output = model(image)\n",
        "            class_probs = torch.softmax(softmax_output, dim=1)\n",
        "            objectness = sigmoid_output.squeeze(1)\n",
        "            max_probs = class_probs.max(dim=1)[0]\n",
        "            nonconf = 1.0 - max_probs\n",
        "\n",
        "            uncertainty_mask = (nonconf > lambda_hat).float()\n",
        "            object_mask = (objectness > obj_thresh).float()\n",
        "            uos_map = (uncertainty_mask * object_mask).cpu().numpy()[0]\n",
        "\n",
        "            pred_mask = filter_and_threshold(\n",
        "                uos_map,\n",
        "                adaptive=adaptive,\n",
        "                top_percent=top_percent,\n",
        "                threshold=0.1,\n",
        "                open_size=open_size,\n",
        "                close_size=close_size,\n",
        "                max_area_ratio=max_area_ratio,\n",
        "                dilate=dilate,\n",
        "                dilate_size=dilate_size\n",
        "            )\n",
        "\n",
        "            img_np = image.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "            img_np = img_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "            img_np = np.clip(img_np, 0, 1)\n",
        "            img_uint8 = (img_np * 255).astype(np.uint8)\n",
        "\n",
        "            gt_resized = cv2.resize(gt_mask.astype(np.uint8), (img_uint8.shape[1], img_uint8.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            pred_resized = cv2.resize(pred_mask.astype(np.uint8), (img_uint8.shape[1], img_uint8.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # pixel-wise metrics\n",
        "            intersection = np.logical_and(gt_resized == 1, pred_resized == 1).sum()\n",
        "            union = np.logical_or(gt_resized == 1, pred_resized == 1).sum()\n",
        "            gt_area = (gt_resized == 1).sum()\n",
        "            pred_area = (pred_resized == 1).sum()\n",
        "\n",
        "            precision = intersection / (pred_area + 1e-6)\n",
        "            recall = intersection / (gt_area + 1e-6)\n",
        "            iou = intersection / (union + 1e-6)\n",
        "\n",
        "            metrics.append({\n",
        "                \"image_id\": idx,\n",
        "                \"GT_pixels\": int(gt_area),\n",
        "                \"UOS_pixels\": int(pred_area),\n",
        "                \"TP\": int(intersection),\n",
        "                \"Precision\": precision,\n",
        "                \"Recall\": recall,\n",
        "                \"IoU\": iou\n",
        "            })\n",
        "\n",
        "            all_preds.append(pred_resized.flatten())\n",
        "            all_targets.append(gt_resized.flatten())\n",
        "\n",
        "            # Overlay\n",
        "            overlay = np.zeros_like(img_uint8)\n",
        "            overlay[gt_resized > 0] = [255, 0, 0]\n",
        "            overlay[pred_resized > 0] = [0, 255, 0]\n",
        "            overlay[(gt_resized > 0) & (pred_resized > 0)] = [255, 255, 0]\n",
        "            blended = cv2.addWeighted(img_uint8, 1.0, overlay, 0.5, 0)\n",
        "\n",
        "            cv2.imwrite(os.path.join(save_dir, \"overlays\", f\"{idx:03d}_overlay.png\"), blended[..., ::-1])\n",
        "            plt.imsave(os.path.join(save_dir, \"maps\", f\"{idx:03d}_uos_raw.png\"), uos_map, cmap=\"hot\")\n",
        "            plt.imsave(os.path.join(save_dir, \"maps\", f\"{idx:03d}_uos_mask.png\"), pred_resized, cmap=\"gray\")\n",
        "\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame(metrics)\n",
        "    df.to_csv(os.path.join(save_dir, \"metrics.csv\"), index=False)\n",
        "\n",
        "    # Global metrics\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    try:\n",
        "        auroc = roc_auc_score(all_targets, all_preds)\n",
        "        ap = average_precision_score(all_targets, all_preds)\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(all_targets, all_preds)\n",
        "        fpr95 = 1 - precision_curve[np.argmin(np.abs(recall_curve - 0.95))] if any(recall_curve >= 0.95) else None\n",
        "    except Exception as e:\n",
        "        auroc, ap, fpr95 = None, None, None\n",
        "        print(f\" AUROC/AP error: {e}\")\n",
        "\n",
        "    tp_total = df[\"TP\"].sum()\n",
        "    gt_total = df[\"GT_pixels\"].sum()\n",
        "    pred_total = df[\"UOS_pixels\"].sum()\n",
        "\n",
        "    precision_global = tp_total / (pred_total + 1e-6)\n",
        "    recall_global = tp_total / (gt_total + 1e-6)\n",
        "    iou_global = tp_total / (gt_total + pred_total - tp_total + 1e-6)\n",
        "\n",
        "    summary = {\n",
        "        \"AUROC\": auroc,\n",
        "        \"AveragePrecision\": ap,\n",
        "        \"FPR@95TPR\": fpr95,\n",
        "        \"GlobalPrecision\": precision_global,\n",
        "        \"GlobalRecall\": recall_global,\n",
        "        \"GlobalIoU\": iou_global,\n",
        "        \"NumImages\": len(df),\n",
        "        \"NumWithGT\": int((df[\"GT_pixels\"] > 0).sum()),\n",
        "        \"NumZeroTP\": int((df[\"TP\"] == 0).sum()),\n",
        "        \"NumOnlyFP\": int(((df[\"GT_pixels\"] == 0) & (df[\"UOS_pixels\"] > 0)).sum())\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, \"summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=4)\n",
        "\n",
        "    print(f\"\\n Saved metrics to: {save_dir}/metrics.csv and summary.json\")\n",
        "    return df, summary\n"
      ],
      "metadata": {
        "id": "QPcOX-_GxOd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, summary = analyze_uos_predictions_filtered(\n",
        "    model=model,\n",
        "    dataset=test_loader.dataset,\n",
        "    device=device,\n",
        "    lambda_hat=0.6,\n",
        "    obj_thresh=0.1,\n",
        "    top_percent=0.05,\n",
        "    adaptive=True,\n",
        "    open_size=1,\n",
        "    close_size=2,\n",
        "    max_area_ratio=0.05,\n",
        "    dilate=True,\n",
        "    dilate_size=4,\n",
        "    save_dir=\"uos_eval_filtered\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "QWdIATLIF5s8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
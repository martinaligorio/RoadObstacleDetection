{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1zPuFTkzxwGoY7oZJYoRFn3YmGwcsZhSL","authorship_tag":"ABX9TyMdv8umYuWtlvigiy7AzdLW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMBmAyZGaK0C","executionInfo":{"status":"ok","timestamp":1752239938399,"user_tz":-120,"elapsed":23481,"user":{"displayName":"Francesco Maria GERMANO","userId":"08944067668830872995"}},"outputId":"bea743bc-5c59-4468-9a9d-0e35966a172a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:21<00:00,  4.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Calibrated lambda_hat at alpha = 0.03: 0.460778\n","Image shape: torch.Size([1, 3, 512, 1024])\n","First 5 non conformity scores: [0.00264668 0.00264668 0.00308925 0.00461453 0.00747019]\n","Loaded lambda_hat: 0.460778\n","Let's see if these 5 pixels exceed lambda_hat [False False False False False]\n"]}],"source":["#================================================\n","#DEBUG TOGGLE SWITCH\n","\n","def d(*args, **kwargs):\n","    if d.ON:\n","        print(*args, **kwargs)\n","\n","d.ON = True          # <- module-local switch\n","\n","##example usage\n","#d(\"Initial x shape:\", x.shape)\n","#================================================\n","if __name__ == \"__main__\":\n","  try:\n","    import segmentation_models_pytorch as smp\n","  except ImportError:\n","    !pip install -q segmentation_models_pytorch timm\n","    import segmentation_models_pytorch as smp\n","#============================================================0\n","\n","\n","#================================================\n","# NAVIGATION TO GET TO THE FOLDER WITH STUFF INSIDE\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/src\")\n","#=================================================0\n","\n","\n","\n","\n","#=================================================\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","from network.deeplab_dualhead import get_model\n","from dataloader.cityscapes_dataloader import CityscapesDataset\n","#========================================================================\n","\n","\n","\n","#========================================================================\n","# PATHS SETTING\n","calib_images_dir = \"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/Datasets/leftImg8bit_trainvaltest/leftImg8bit_trainvaltest/leftImg8bit/val\"\n","calib_masks_dir = \"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/Datasets/leftImg8bit_trainvaltest/gtFine_trainvaltest/gtFine/val\"\n","split_dir = \"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/splits\"\n","\n","with open(os.path.join(split_dir, \"calib_cp.txt\"), \"r\") as f:\n","  calib_list = [line.strip() for line in f]\n","#===========================================================================\n","\n","\n","#============================================\n","# TAKIN DATASET\n","calib_dataset = CityscapesDataset(calib_images_dir, calib_masks_dir, calib_list, augment=False) #ofc augm was just for training\n","calib_loader = DataLoader(calib_dataset, batch_size=1, shuffle=False, num_workers=2) #now i want data to be provided one at a time\n","#==============================================\n","\n","\n","#===============================================================\n","# SETTING UP THE MODEL\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = get_model().to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/ckpts/old_model.pth\"))\n","model.eval()\n","#===================================================================================0\n","\n","\n","#====================================================================================\n","# NON CONFORMITY SCORES COMPUTATION\n","scores = []\n","with torch.no_grad(): #deactivation of gradient tracking as well for non training purposes\n","  for images, _, _ in tqdm(calib_loader):\n","    images = images.to(device)\n","    pred_softmax, _ = model(images)\n","    pred_softmax = F.interpolate(pred_softmax, size=images.shape[-2:], mode=\"bilinear\", align_corners=False) #outpu has reduced dimension remember\n","    max_probs = torch.max(torch.softmax(pred_softmax, dim=1), dim=1)[0] #takin max prob value for each px\n","    nonconf = 1.0 - max_probs #non conformity score\n","    scores.append(nonconf.cpu().numpy())\n","\n","scores = np.concatenate([s.flatten() for s in scores], axis=0) #take all the arrays in scores variable, flatten in 1D and merging in one array with all of em\n","#====================================================================================\n","\n","\n","\n","#=========================================================================================\n","# CONFORMAL CALIBRATION\n","alpha = 0.03\n","lambda_hat = np.quantile(scores, 1-alpha) #1-alpha to ensure the 97% of those values are under this treshold -> indicates how much uncertainty has THAT px if it goes over\n","print(f\"Calibrated lambda_hat at alpha = {alpha}: {lambda_hat:.6f}\")\n","\n","with open(\"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/src/eval/lambda_hat.txt\", \"w\") as f:\n","  f.write(f\"{lambda_hat:.6f}\")\n","  f.write(\"\\n\")\n","#=========================================================================================\n","\n","\n","\n","##########################################################################################################\n","# DUMMY TEST JUST WITH 5 PIXELS FROM FIRST IMAGE\n","\n","if __name__ == \"__main__\":\n","  sample_image, _, _ = calib_dataset[0]\n","  sample_image = sample_image.unsqueeze(0).to(device)\n","\n","  with torch.no_grad():\n","    pred_softmax, _ = model(sample_image)\n","    pred_softmax = F.interpolate(pred_softmax, size=sample_image.shape[-2:], mode=\"bilinear\", align_corners=False)\n","    max_probs = torch.max(torch.softmax(pred_softmax, dim=1), dim=1)[0]\n","    nonconf = 1.0 - max_probs\n","\n","  d(\"Image shape:\", sample_image.shape)\n","  flat_scores = nonconf.view(-1)\n","  d(\"First 5 non conformity scores:\", flat_scores[:5].cpu().numpy())\n","\n","  with open(\"/content/drive/MyDrive/MAGISTRALE/ANNO 1/Computer Vision/Project/RoadObstacleDetection/ProjectWorkspace/src/eval/lambda_hat.txt\", \"r\") as f:\n","    lambda_hat_loaded = float(f.readline().strip())\n","\n","  d(\"Loaded lambda_hat:\", lambda_hat_loaded)\n","\n","  exceed = flat_scores[:5] > lambda_hat_loaded\n","  d(\"Let's see if these 5 pixels exceed lambda_hat\", exceed.cpu().numpy())\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}
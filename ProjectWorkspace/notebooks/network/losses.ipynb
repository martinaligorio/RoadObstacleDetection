{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WNVr9cNF6kgwaP2Zu3JW9j-cJGg7ZRMM","authorship_tag":"ABX9TyMuCBS349U0r7o3Len9/OUp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"h-SYJ23uDddZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752321872158,"user_tz":-120,"elapsed":5488,"user":{"displayName":"Francesco Maria GERMANO","userId":"08944067668830872995"}},"outputId":"c8821610-8978-42f2-94b7-ef86d050df3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss cross entropy: 0.009174452163279057\n","Loss BINARY cross entropy (object_mask): 0.04858735203742981\n","LOSS TOTAL: 0.1063491553068161\n","\n","Final dummy test LOSS = 0.106349\n"]}],"source":["#================================================\n","#DEBUG TOGGLE SWITCH\n","\n","def d(*args, **kwargs):\n","    if d.ON:\n","        print(*args, **kwargs)\n","\n","d.ON = False          # <- module-local switch\n","\n","##example usage\n","#d(\"Initial x shape:\", x.shape)\n","#================================================\n","\n","\n","\n","#===========================================================000\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#import cv2 EDIT: was giving problems in inference phase\n","import numpy as np\n","\n","#=====================================================================\n","# LOSS FUNCTION GETTER\n","\n","def get_loss(lambda_bce=3.0):  #factory function that returns a customised loss_fn with lambda = 3.0\n","  ce_loss_fn = nn.CrossEntropyLoss() #this here is for the softmax head, will work on pred_softmax and target_labels (find em below)\n","  bce_loss_fn = nn.BCEWithLogitsLoss() #this here for the sigmoid head instead; mask on edges to be applied later\n","\n","  def loss_fn(pred_softmax, pred_sigmoid, target_labels, target_objectness): #this in fact is the real loss function\n","    loss_ce = ce_loss_fn(pred_softmax, target_labels) #for semantic segmentation, cross entropy; WATCH OUT! target_labels are the ground truth classes\n","\n","    #BCE on object boundaries\n","    #boundary_masks =  [] #first we gotta compute the boundary mask 1 where label edges are present; EDIT: giving problems in inference\n","\n","    \"\"\" #EDIT: was giving problems in inference\n","    for label in target_labels.cpu().numpy():\n","      label_uint8 = label.astype(np.uint8)\n","      edge = cv2.Canny(label_uint8, 0, 1) #binary edge map, very first lesson for theory, edges have 255\n","      dilated = cv2.dilate(edge, np.ones((3,3), np.uint8), iterations=1) #stretches borders of 3px with 3x3 krnl\n","      boundary_masks.append(dilated / 255.0) #result normalization (0,255) -> (0,1)\n","    \"\"\"\n","\n","    #Stacking and conversion to a tensor\n","    #boundary_mask_tensor = torch.from_numpy(np.stack(boundary_masks)).to(target_objectness.device).float() #boundary mask recombined as a tensor; stack is to send it to GPU as a batch tensor; EDIT: was giving problems in inference\n","\n","    #makin sure shape is [B, 1, H, W] as well as the pred_sigmoid\n","    #boundary_mask_tensor = boundary_mask_tensor.unsqueeze(1) #changing shape to multiply it with bce; EDIT: was giving problems in inference\n","\n","\n","\n","    #raw pixel-wise BCE computation\n","    pred_sigmoid = torch.clamp(pred_sigmoid, min=-10.0, max=10.0)\n","    #pred_sigmoid = pred_sigmoid.squeeze(1)\n","    #target_objectness = target_objectness.unsqueeze(1)\n","\n","    loss_bce = bce_loss_fn(pred_sigmoid, target_objectness) #pixel-wies but still unmasked\n","\n","    #boundary mask application: loss only on edges\n","    #bce_masked = bce_raw * boundary_mask_tensor #EDIT: giving problems in inference\n","\n","    #taking the mean over all the boundary pixels (shouldnt divide by total pixels)\n","    #loss_bce = bce_masked.sum() / (boundary_mask_tensor.sum() + 1e-2) #1e-6 needed to avoid division by 0 if batch has no edges, EDIT: 1e-6 was too high, tried to lift up at 1e-2 to see if avoids loss explosion; EDIT: was giving problems in inference\n","\n","    #computing total\n","    loss_total = loss_ce + lambda_bce * loss_bce #combining the two to handle them together\n","\n","    d(\"Loss cross entropy:\", loss_ce.item())\n","    d(\"Loss BINARY cross entropy (object_mask):\", loss_bce.item())\n","    d(\"LOSS TOTAL:\", loss_total.item())\n","\n","    return loss_total #to be passed to the backward pass\n","\n","  return loss_fn\n","\n","#========================================================================================================================================================\n","# DUMMY NUMERIC TEST (EDITED for full-mask objectness loss)\n","\n","if __name__ == \"__main__\":\n","    d.ON = True\n","\n","    # Dummy dimensions\n","    B, H, W = 1, 2, 2\n","    n_classes = 3\n","    lambda_value = 2.0\n","\n","    # Simulated logits for semantic segmentation\n","    pred_softmax = torch.tensor([[[[ 4.0,  4.0],\n","                                   [ 4.0,  4.0]],\n","                                  [[-1.0, -1.0],\n","                                   [-1.0, -1.0]],\n","                                  [[-2.0, -2.0],\n","                                   [-2.0, -2.0]]]], dtype=torch.float32)  # shape [1, 3, 2, 2]\n","\n","    # Ground truth segmentation (all class 0)\n","    target_labels = torch.zeros((B, H, W), dtype=torch.long)  # shape [1, 2, 2]\n","\n","    # Simulated logits for objectness\n","    pred_sigmoid = torch.tensor([[[[3.0, 3.0],\n","                                   [3.0, 3.0]]]], dtype=torch.float32)  # shape [1, 1, 2, 2]\n","\n","    # Ground truth objectness mask (all pixels = object)\n","    target_objectness = torch.ones((B, 1, H, W), dtype=torch.float32)  # shape [1, 1, 2, 2]\n","\n","    # Send to device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    pred_softmax = pred_softmax.to(device)\n","    pred_sigmoid = pred_sigmoid.to(device)\n","    target_labels = target_labels.to(device)\n","    target_objectness = target_objectness.to(device)\n","\n","    # Compute loss\n","    criterion = get_loss(lambda_bce=lambda_value)\n","    loss = criterion(pred_softmax, pred_sigmoid, target_labels, target_objectness)\n","\n","    d(f\"\\nFinal dummy test LOSS = {loss.item():.6f}\")\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jkfkqra-Ze5TJ70ogiRJE9qNauVzr_FW","authorship_tag":"ABX9TyPEguHuGIeVi8RRWszKJ4PX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hhzor_B6j82","executionInfo":{"status":"ok","timestamp":1752077434710,"user_tz":-120,"elapsed":119747,"user":{"displayName":"Francesco Maria GERMANO","userId":"08944067668830872995"}},"outputId":"b5f3ad93-d971-4bfb-e06b-a74a9cc2a2be"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["#================================================\n","#DEBUG TOGGLE SWITCH\n","\n","def d(*args, **kwargs):\n","    if d.ON:\n","        print(*args, **kwargs)\n","\n","d.ON = False          # <- module-local switch\n","\n","##example usage\n","#d(\"Initial x shape:\", x.shape)\n","#================================================\n","\n","\n","#===============================================================\n","\n","\n","#===============================================================\n","#================================================\n","#WATCH OUT! ONLY EXECUTABLE IN THE ACTUAL NOTEBOOK!!!! SO MAKE SURE IT'S COMMENTED WHEN YOU EXPORT IS AS .py!!!! next time do that thing you do for dummy tests (name = main)\n","\"\"\"\n","try:\n","    import segmentation_models_pytorch as smp\n","except ImportError:\n","    !pip install -q segmentation-models-pytorch\n","    !pip install -q timm\n","    import segmentation_models_pytorch as smp\n","\"\"\"\n","\n","\n","#========================================================\n","\n","import torch\n","import torch.nn as nn\n","import segmentation_models_pytorch as smp\n","\n","#======================================================\n","\n","# CLASS DEFINITION\n","class DeepLabDualHead(nn.Module):\n","  def __init__(self, n_classes=7):\n","    super().__init__()\n","\n","    self.base = smp.DeepLabV3Plus( #base model DeepLabV3+ kept as it is for ENCODER+DECODER, but gotta override the head to introduce novelty\n","       encoder_name=\"resnet50\",\n","       encoder_weights=\"imagenet\",\n","       in_channels=3,\n","       classes=n_classes #will get overridden by novelty trait planned\n","    )\n","\n","    self.base.classifier = nn.Identity() # removing the built-in classification head and just keepin encoder/decoder arch\n","\n","    #SEGMENTATION HEAD\n","    self.head_softmax = nn.Conv2d(256, n_classes, kernel_size=1)  #256 is the number of input channels in the layer, n_classes as the number of predictions we want for each pixel (probability for each class ofc), 1 as k.size means each convolution is computed independently pixel for pixel\n","\n","    #OBJECTNESS HEAD\n","    self.head_sigmoid = nn.Sequential(\n","        nn.Conv2d(256, 1, kernel_size=1), #256 is the number of input channels in the layer, 1 as the single prediction we want for each pixel (obj/!obj), 1 as k.size means each convolution is computed independently pixel for pixel\n","        nn.Sigmoid()\n","    )\n","\n","  def forward(self, x):\n","    features = self.base.encoder(x) #input goes through encoder, features'lists get produced\n","    decoder_output = self.base.decoder(features) #features'list go through decoder, has to find out the semantic output EDIT (error in training loop): DeepLabV3PlusDecoder takes 2 positional arguments but 7 were given, so i can't destructure the list and pass separated arguments, but the whole one as it is\n","\n","    #SEMANTIC PREDICTION OUTPUT CONSTRUCTION\n","    softmax_output = self.head_softmax(decoder_output)\n","\n","    #OBJECTNESS PREDICTION OUTPUT CONSTRUCTION\n","    sigmoid_output = self.head_sigmoid(decoder_output)\n","\n","    return softmax_output, sigmoid_output\n","\n","#============================================================\n","\n","\n","#==============================================================\n","#way to export it\n","def get_model():\n","  return DeepLabDualHead()\n","#=============================================================="]}]}